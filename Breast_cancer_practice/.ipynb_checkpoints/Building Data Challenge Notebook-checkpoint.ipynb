{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge Notebook\n",
    "\n",
    "This notebook serves as a repertoire of python functions for solving data challenges, includes 10 tips for how to be successful, and how to develop metrics for analytical based projects. \n",
    "\n",
    "Most data challenges evaluate coding level, and ability to inspect data (i.e. identifying paradox, errors), perform data wrangling, choose appropriate visualizations, apply statistics/ML and concisely and effectively answer questions. \n",
    "\n",
    "This notebook includes functions for high level processes that may be necessary for a given data challenge and is meant to be customized for your particular challenge. The sections include:\n",
    "\n",
    "- Initial Data Analysis\n",
    "- Data Wrangling\n",
    "- Exploratory Data Analysis\n",
    "- Statistical Analysis [Optional]\n",
    "- Machine Learning [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips & Tricks:\n",
    "1. Create a notebook template of custom functions for analysis, visualizations and modeling BEFORE starting challenge!\n",
    "2. Research the company/role. (Many companies will utilize their own data for challenges)\n",
    "3. Develop a plan of action prior to coding and ask for clarification on the prompt and data, if necessary.\n",
    "4. Keep track of time (Some challenges allot only a few hours!)\n",
    "5. Use functional programming whenever possible for conciseness. (Show all your work!)\n",
    "6. Use intuitive dataframe names-not \"df\".\n",
    "7. State all assumptions! (Prompts are purposely vague, you will have to make decisions based on the information provided).\n",
    "8. Explain all values/visualizations/errors/results as it pertains to the prompt provided. (Think storytelling)\n",
    "9. Outline next steps and actionable insights dervied from analyses.\n",
    "10. Add comments and utilize markdown to explain functions (input/output) and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing metrics for analysis\n",
    "Consider what 'health' and success means to the particular business and what information would provide useful insights.\n",
    "- What is their business model?\n",
    "- Who is their target audience?\n",
    "- What would they want to optimize?\n",
    "\n",
    "A good metric should be:\n",
    "- Comparative\n",
    "- Understandable\n",
    "- Ratio/Rate\n",
    "- Changes the way you behave\n",
    "    \n",
    "Types of metrics, include:\n",
    "- Qualitative/Quantitative\n",
    "- Vanity/Actionable\n",
    "- Exploratory/Reporting\n",
    "- Leading/Lagging\n",
    "- Correlated/Causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Company Name]\n",
    "\n",
    "### Data Challenge\n",
    "### Data & Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Initial Data Analysis\n",
    "- Data Wrangling\n",
    "- Exploratory Data Analysis\n",
    "- Statistical Analysis\n",
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas_profiling as pp\n",
    "import etsy_py\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_analysis(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe produces a simple report on initial data analytics\n",
    "    Params:\n",
    "        - df \n",
    "    Returns:\n",
    "        - Shape of dataframe records and columns\n",
    "        - Columns and data types\n",
    "    \"\"\"\n",
    "    print('Report of Initial Data Analysis:\\n')\n",
    "    print(f'Shape of dataframe: {df.shape}')\n",
    "    print(f'Features and Data Types: \\n {df.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe it calculates the percentage of missing records per column\n",
    "    Params:\n",
    "        - df\n",
    "    Returns:\n",
    "        - Dictionary of column name and percentage of missing records\n",
    "    \"\"\"\n",
    "    col=list(df.columns)\n",
    "    perc=[round(df[c].isna().mean()*100,2) for c in col]\n",
    "    miss_dict=dict(zip(col,perc))\n",
    "    return miss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test(df,col_list):\n",
    "    \"\"\"\n",
    "    Given a dataframe determines whether each numerical column is Gaussian \n",
    "    Ho = Assumes distribution is not Gaussian\n",
    "    Ha = Assumes distribution is Gaussian\n",
    "    Params:\n",
    "        - df\n",
    "    Returns:\n",
    "        - W Statistic\n",
    "        - p-value\n",
    "        - List of columns that do not have gaussian distribution\n",
    "    \"\"\"\n",
    "    non_gauss=[]\n",
    "    w_stat=[]\n",
    "    # Determine if each sample of numerical feature is gaussian\n",
    "    alpha = 0.05\n",
    "    for n in numeric_list:\n",
    "        stat,p=shapiro(df[n])\n",
    "        print(sns.distplot(df[n]))\n",
    "        print(tuple(skew(df[n]),kurtosis(df[n])))\n",
    "\n",
    "        if p <= alpha: # Reject Ho -- Distribution is not normal\n",
    "            non_gauss.append(n)\n",
    "            w_stat.append(stat)\n",
    "    # Dictionary of numerical features not gaussian and W-Statistic        \n",
    "    norm_dict=dict(zip(non_gauss,w_stat))\n",
    "    return norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
